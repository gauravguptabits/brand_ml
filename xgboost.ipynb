{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/simran.tyagi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score,f1_score,recall_score,precision_score, fbeta_score\n",
    "import xgboost as xgb\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ftfy\n",
    "#!pip install scrubadub\n",
    "#!pip install demoji\n",
    "#!pip install tabulate\n",
    "#!pip install wordcloud\n",
    "#pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file():\n",
    "    df = pd.read_csv('/home/simran.tyagi/Downloads/panasonic_v1_f.csv')\n",
    "    df = df[['text', 'Complaint']]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------  -----------  ------------\n",
      "Step                Total words  Unique words\n",
      "Start               77684        20298\n",
      "Lower               77684        18224\n",
      "Lemmatize           77684        17402\n",
      "Unicode Fix         77691        17336\n",
      "Replace emoji       78144        17451\n",
      "Stop words          59345        16833\n",
      "Email Replace       59345        16826\n",
      "UserName replace    60034        15499\n",
      "HashTags Replace    75185        11377\n",
      "URL Replace         75185        9179\n",
      "MARKUP Replace      75185        9178\n",
      "Remove punctuation  75185        7210\n",
      "------------------  -----------  ------------\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import ftfy\n",
    "import scrubadub\n",
    "# import string\n",
    "from tabulate import tabulate\n",
    "import demoji\n",
    "# import emoji\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "emailDetector = scrubadub.Scrubber(detector_list=[scrubadub.detectors.EmailDetector])\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "label_codes = {'No': 0, 'Yes': 1}\n",
    "t_handle_regex = r'(^|[^@\\w])@(\\w{1,15})\\b'\n",
    "t_hashtag_regex = r\"#(\\w+)\"\n",
    "t_url_regex = r\"https?://\\S+|www\\.\\S+\"\n",
    "t_markup_regex = r\"<(\\\"[^\\\"]*\\\"|'[^']*'|[^'\\\">])*>\"\n",
    "t_handle_placeholder = ' {{HANDLE}}'\n",
    "t_hashtag_placeholder = ' {{HASHTAG}}'\n",
    "t_url_placeholder = '{{URL}}'\n",
    "t_markup_placeholder = '{{MARKUP}}'\n",
    "emoji_placeholder = '{{EMOJI}}'\n",
    "# domain specific stopwords.\n",
    "stop.extend(['panasonic'])\n",
    "\n",
    "# table = str.maketrans(\"\", \"\")\n",
    "\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    def is_noun(tag):\n",
    "        return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "    def is_verb(tag):\n",
    "        return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "    def is_adverb(tag):\n",
    "        return tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "    def is_adjective(tag):\n",
    "        return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "    # Pos tags to wn tags\n",
    "    if is_adjective(tag):\n",
    "        return wn.ADJ\n",
    "    elif is_noun(tag):\n",
    "        return wn.NOUN\n",
    "    elif is_adverb(tag):\n",
    "        return wn.ADV\n",
    "    elif is_verb(tag):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "def to_lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "def fix_unicode(text):\n",
    "    return ftfy.fix_text(text)\n",
    "\n",
    "def replace_email(text):\n",
    "    return emailDetector.clean(text)\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    return ' '.join([word for word in text.split() if word not in (stop)])\n",
    "\n",
    "def convert_emoji_to_text(text):\n",
    "    return text\n",
    "\n",
    "def replace_user_name(text):\n",
    "    return re.sub(t_handle_regex, t_handle_placeholder, text)\n",
    "\n",
    "def replace_hashtags(text):\n",
    "    return re.sub(t_hashtag_regex, t_hashtag_placeholder,text)\n",
    "\n",
    "def replace_url(text):\n",
    "    return re.sub(t_url_regex, t_url_placeholder, text)\n",
    "\n",
    "def replace_markup(text):\n",
    "    return re.sub(t_markup_regex, t_markup_placeholder,text)\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "def replace_emoji_with_code(text):\n",
    "    demoji.replace(text, repl=emoji_placeholder)\n",
    "    return demoji.replace_with_desc(text)\n",
    "\n",
    "def get_stats(step, df):\n",
    "    corpus = \" \".join(list(df['text']))\n",
    "    total_words = len(corpus.split(' '))\n",
    "    unique_words = len(set(corpus.split(' ')))\n",
    "    return [step, total_words, unique_words]\n",
    "\n",
    "def lemmatize(text):\n",
    "    default_wn_tag = 'n'\n",
    "    tokens = text.split(' ')\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    wn_tags = [penn_to_wn(tag) for (w, tag) in pos_tags]\n",
    "    # print(list(zip(pos_tags, wn_tags)))\n",
    "    lemmas = [wnl.lemmatize(token, tag or default_wn_tag) for (token, tag) in list(zip(tokens, wn_tags))]\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "stats = [['Step', 'Total words', 'Unique words']]\n",
    "df = read_file()\n",
    "stats.append(get_stats('Start', df))\n",
    "df = df.replace(label_codes)\n",
    "\n",
    "df['text'] = df['text'].apply(lambda text: text.lower())\n",
    "stats.append(get_stats('Lower', df))\n",
    "\n",
    "df['text'] = df['text'].apply(lambda text: lemmatize(text))\n",
    "stats.append(get_stats('Lemmatize', df))\n",
    "\n",
    "df['text'] = df['text'].apply(fix_unicode)\n",
    "stats.append(get_stats('Unicode Fix', df))\n",
    "\n",
    "df['text'] = df['text'].apply(replace_emoji_with_code)\n",
    "stats.append(get_stats('Replace emoji', df))\n",
    "\n",
    "df['text'] = df['text'].apply(remove_stop_words)\n",
    "stats.append(get_stats('Stop words', df))\n",
    "\n",
    "df['text'] = df['text'].apply(replace_email)\n",
    "stats.append(get_stats('Email Replace', df))\n",
    "\n",
    "df['text'] = df['text'].apply(replace_user_name)\n",
    "stats.append(get_stats('UserName replace', df))\n",
    "\n",
    "df['text'] = df['text'].apply(replace_hashtags)\n",
    "stats.append(get_stats('HashTags Replace', df))\n",
    "\n",
    "df['text'] = df['text'].apply(replace_url)\n",
    "stats.append(get_stats('URL Replace', df))\n",
    "\n",
    "df['text'] = df['text'].apply(replace_markup)\n",
    "stats.append(get_stats('MARKUP Replace', df))\n",
    "\n",
    "df['text'] = df['text'].apply(remove_punctuations)\n",
    "stats.append(get_stats('Remove punctuation', df))\n",
    "\n",
    "\n",
    "print(tabulate(stats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf = TfidfVectorizer(lowercase=False, max_df=.8, min_df=0.01)  \n",
    "tfidf_wm = tfidf.fit_transform(df['text']).toarray() \n",
    "x_train, x_test, y_train, y_test = train_test_split(tfidf_wm, df['Complaint'], random_state = 42) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:30:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "model_x = xgb.XGBClassifier(use_label_encoder=False,\n",
    " learning_rate = 0.35, ## helps in avoding the overfitting problem and increase in learning rate affects TP and FN                            \n",
    " n_estimators=56,## number of trees to be built and increase in n_estimators affects TP and FP\n",
    " max_depth=3,##  represents the depth of each tree and increase in max_depth affects TN and FN\n",
    " subsample=0.8, # for each tree the % of rows taken to build the tree and increase in subsample affects TN and FN \n",
    " min_child_weight=1, ##Defines the minimum sum of weights of all observations required in a child and increase in min_child_weight affects TN and FN \n",
    " reg_alpha = 0.6,## penalizes the features which increase cost function and high value of reg_alpha affects TN and FN rate \n",
    " reg_lambda = 0.6,## encourages the weights to be small and increase in reg_lambda affects TN and FN\n",
    " seed = 42).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.957\n",
      "Recall-Score: 0.817\n",
      "Precision-Score: 0.873\n",
      "F1-score: 0.844\n",
      "F2-Score: 0.83\n"
     ]
    }
   ],
   "source": [
    "pred_test_x = model_x.predict(x_test)\n",
    "print(\"Accuracy Score:\",round(accuracy_score(y_test,pred_test_x),3))\n",
    "print(\"Recall-Score:\",round(recall_score(y_test,pred_test_x),3))\n",
    "print(\"Precision-Score:\",round(precision_score(y_test,pred_test_x),3))\n",
    "print(\"F1-score:\",round(f1_score(y_test,pred_test_x),3))\n",
    "print(\"F2-Score:\",round(fbeta_score(y_test,pred_test_x,beta = 2),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "\n",
      " [[640  13]\n",
      " [ 20  89]]\n",
      "\n",
      "True Positives(TP) =  640\n",
      "\n",
      "True Negatives(TN) =  89\n",
      "\n",
      "False Positives(FP) =  13\n",
      "\n",
      "False Negatives(FN) =  20\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, pred_test_x)\n",
    "print('Confusion matrix\\n\\n', c_matrix)\n",
    "\n",
    "print('\\nTrue Positives(TP) = ', c_matrix[0,0])\n",
    "\n",
    "print('\\nTrue Negatives(TN) = ', c_matrix[1,1])\n",
    "\n",
    "print('\\nFalse Positives(FP) = ', c_matrix[0,1])\n",
    "\n",
    "print('\\nFalse Negatives(FN) = ', c_matrix[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e490422cd5eb80d883ca3c84d0761f55a6ef3e6322675b57b784cf7be9fc5aec"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
