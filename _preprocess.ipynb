{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import ftfy\n",
    "import scrubadub\n",
    "# import string\n",
    "from tabulate import tabulate\n",
    "import demoji\n",
    "# import emoji\n",
    "from nltk.corpus import stopwords\n",
    "import contractions\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "emailDetector = scrubadub.Scrubber(\n",
    "    detector_list=[scrubadub.detectors.EmailDetector])\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "label_codes = {'No': 0, 'Yes': 1}\n",
    "t_handle_regex = r'(^|[^@\\w])@(\\w{1,15})\\b'\n",
    "t_hashtag_regex = r\"#(\\w+)\"\n",
    "t_url_regex = r\"https?://\\S+|www\\.\\S+\"\n",
    "t_markup_regex = r\"<(\\\"[^\\\"]*\\\"|'[^']*'|[^'\\\">])*>\"\n",
    "t_handle_placeholder = ' {{HANDLE}}'\n",
    "t_hashtag_placeholder = ' {{HASHTAG}}'\n",
    "t_url_placeholder = '{{URL}}'\n",
    "t_markup_placeholder = '{{MARKUP}}'\n",
    "emoji_placeholder = '{{EMOJI}}'\n",
    "# domain specific stopwords.\n",
    "stop.extend(['panasonic'])\n",
    "\n",
    "# table = str.maketrans(\"\", \"\")\n",
    "\n",
    "\n",
    "def read_file(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df[['text', 'Complaint']]\n",
    "    return df\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    def is_noun(tag):\n",
    "        return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "    def is_verb(tag):\n",
    "        return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "    def is_adverb(tag):\n",
    "        return tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "    def is_adjective(tag):\n",
    "        return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "    # Pos tags to wn tags\n",
    "    if is_adjective(tag):\n",
    "        return wn.ADJ\n",
    "    elif is_noun(tag):\n",
    "        return wn.NOUN\n",
    "    elif is_adverb(tag):\n",
    "        return wn.ADV\n",
    "    elif is_verb(tag):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "\n",
    "def to_lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def fix_unicode(text):\n",
    "    return ftfy.fix_text(text)\n",
    "\n",
    "\n",
    "def replace_email(text):\n",
    "    return emailDetector.clean(text)\n",
    "\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    return ' '.join([word for word in text.split() if word not in (stop)])\n",
    "\n",
    "\n",
    "def convert_emoji_to_text(text):\n",
    "    return text\n",
    "\n",
    "\n",
    "def replace_user_name(text):\n",
    "    return re.sub(t_handle_regex, t_handle_placeholder, text)\n",
    "\n",
    "\n",
    "def replace_hashtags(text):\n",
    "    return re.sub(t_hashtag_regex, t_hashtag_placeholder, text)\n",
    "\n",
    "\n",
    "def replace_url(text):\n",
    "    return re.sub(t_url_regex, t_url_placeholder, text)\n",
    "\n",
    "\n",
    "def replace_markup(text):\n",
    "    return re.sub(t_markup_regex, t_markup_placeholder, text)\n",
    "\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "\n",
    "def replace_emoji_with_code(text):\n",
    "    demoji.replace(text, repl=emoji_placeholder)\n",
    "    return demoji.replace_with_desc(text)\n",
    "\n",
    "\n",
    "def get_stats(step, df):\n",
    "    corpus = \" \".join(list(df['text']))\n",
    "    total_words = len(corpus.split(' '))\n",
    "    unique_words = len(set(corpus.split(' ')))\n",
    "    return [step, total_words, unique_words]\n",
    "\n",
    "def lemmatize(text):\n",
    "    default_wn_tag = 'n'\n",
    "    tokens = text.split(' ')\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    wn_tags = [penn_to_wn(tag) for (w, tag) in pos_tags]\n",
    "    # print(list(zip(pos_tags, wn_tags)))\n",
    "    lemmas = [wnl.lemmatize(token, tag or default_wn_tag)\n",
    "              for (token, tag) in list(zip(tokens, wn_tags))]\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "def fix_contractions(text):\n",
    "    return contractions.fix(text, slang=False)\n",
    "\n",
    "def trim_excessive_space(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "# TODO: https://www.geeksforgeeks.org/spelling-checker-in-python/\n",
    "def spell_correct(text):\n",
    "    textBlb = TextBlob(text)            # Making our first textblob\n",
    "    textCorrected = textBlb.correct()\n",
    "    return textCorrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocesses the data.\n",
    "\"\"\"\n",
    "def process_data(df, **kwargs):\n",
    "    stats = [['Step', 'Total words', 'Unique words']]\n",
    "    stats.append(get_stats('Start', df))\n",
    "    df = df.replace(label_codes)\n",
    "    df['orig_text'] = df['text']\n",
    "\n",
    "    if kwargs.get('handle_retweet'):\n",
    "        df = df[~df['text'].str.startswith('RT')]\n",
    "        stats.append(get_stats('Remove Retweet', df))\n",
    "    else:\n",
    "        stats.append(['Remove Retweet', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "    if kwargs.get('handle_case'):\n",
    "        df['text'] = df['text'].apply(lambda text: text.lower())\n",
    "        stats.append(get_stats('Lower', df))\n",
    "    else:\n",
    "        stats.append(['Lower', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "    if kwargs.get('handle_contractions'):\n",
    "        df['text'] = df['text'].apply(lambda text: fix_contractions(text))\n",
    "        stats.append(get_stats('Remove Retweet', df))\n",
    "    else:\n",
    "        stats.append(['Remove Retweet', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "\n",
    "    if kwargs.get('handle_lemmatization'):\n",
    "        df['text'] = df['text'].apply(lambda text: lemmatize(text))\n",
    "        stats.append(get_stats('Lemmatize', df))\n",
    "    else:\n",
    "        stats.append(['Lemmatize', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "    if kwargs.get('handle_unicode'):\n",
    "        df['text'] = df['text'].apply(fix_unicode)\n",
    "        stats.append(get_stats('Unicode Fix', df))\n",
    "    else:\n",
    "        stats.append(['Unicode Fix', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "    if kwargs.get('handle_emoji'):\n",
    "        df['text'] = df['text'].apply(replace_emoji_with_code)\n",
    "        stats.append(get_stats('Replace emoji', df))\n",
    "    else:\n",
    "        stats.append(['Replace emoji', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "    if kwargs.get('handle_stopwords'):\n",
    "        df['text'] = df['text'].apply(remove_stop_words)\n",
    "        stats.append(get_stats('Stop words', df))\n",
    "    else:\n",
    "        stats.append(['Stop words', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "    if kwargs.get('handle_email'):\n",
    "        df['text'] = df['text'].apply(replace_email)\n",
    "        stats.append(get_stats('Email Replace', df))\n",
    "    else:\n",
    "        stats.append(['Email Replace', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "    if kwargs.get('handle_username'):\n",
    "        df['text'] = df['text'].apply(replace_user_name)\n",
    "        stats.append(get_stats('UserName replace', df))\n",
    "    else:\n",
    "        stats.append(['UserName replace', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "    if kwargs.get('handle_hashtags'):\n",
    "        df['text'] = df['text'].apply(replace_hashtags)\n",
    "        stats.append(get_stats('HashTags Replace', df))\n",
    "    else:\n",
    "        stats.append(['HashTags Replace', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "    if kwargs.get('handle_url'):\n",
    "        df['text'] = df['text'].apply(replace_url)\n",
    "        stats.append(get_stats('URL Replace', df))\n",
    "    else:\n",
    "        stats.append(['URL Replace', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "    if kwargs.get('handle_markup'):\n",
    "        df['text'] = df['text'].apply(replace_markup)\n",
    "        stats.append(get_stats('MARKUP Replace', df))\n",
    "    else:\n",
    "        stats.append(['MARKUP Replace', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "    if kwargs.get('handle_spelling'):\n",
    "        df['text'] = df['text'].apply(spell_correct)\n",
    "        stats.append(get_stats('Spell Correct', df))\n",
    "    else:\n",
    "        stats.append(['Spell Correct', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "\n",
    "    if kwargs.get('handle_punctuation'):\n",
    "        df['text'] = df['text'].apply(remove_punctuations)\n",
    "        stats.append(get_stats('Remove punctuation', df))\n",
    "    else:\n",
    "        stats.append(['Remove punctuation', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "\n",
    "    df['text'] = df['text'].apply(lambda t: trim_excessive_space(t))\n",
    "    print(tabulate(stats)) if kwargs.get('print_stats', False) else None\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "import jsonpickle\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, fbeta_score\n",
    "\n",
    "\n",
    "root = '/home/gaurav.gupta/projects/PoCs/brandMention/brand_ml'\n",
    "\n",
    "def get_model_info(model, model_name):\n",
    "    info = {\n",
    "        'name': str(model.__class__),\n",
    "        model_name: model.get_params()\n",
    "    }\n",
    "    return info\n",
    "\n",
    "\n",
    "def get_experiment_info(model_dict, tags):\n",
    "    experiment_info = {\n",
    "        'models_info': [get_model_info(model_dict[m], m) for m in model_dict.keys()],\n",
    "        'tags': tags,\n",
    "        'timestamp': datetime.now().isoformat()}\n",
    "    return experiment_info\n",
    "\n",
    "\n",
    "# UTIL to serialize classifier to disk along with important info.\n",
    "def save_experiment(experiment_name, model_dict, tags, results={}, preprocess_details={}):\n",
    "    folder = os.path.join(root, 'experiments', experiment_name)\n",
    "    # models = list(model_dict.values())\n",
    "    model_names = model_dict.keys()\n",
    "    readme_file_path = os.path.join(folder, 'artifacts.json')\n",
    "    # results_fp = os.path.join(folder, 'results.csv')\n",
    "\n",
    "    experiment_info = get_experiment_info(model_dict, tags)\n",
    "    experiment_info['name'] = experiment_name\n",
    "    experiment_info['results'] = results\n",
    "    experiment_info['preprocess_config'] = preprocess_details\n",
    "\n",
    "    # Write models.\n",
    "    os.makedirs(folder)\n",
    "    [pickle.dump(model_dict[name], open(os.path.join(\n",
    "        folder, f'{name}.pickle'), 'wb')) for name in model_names]\n",
    "\n",
    "    frozen = jsonpickle.encode(experiment_info, indent=4)\n",
    "    with open(readme_file_path, \"w\") as outfile:\n",
    "        outfile.write(frozen)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTIL to load classifier from disk\n",
    "def load_model(experiment_name):\n",
    "    # pickle.dump(tfidf, open(\"./models/tfidf_rf_f1_9371.pickle\", \"wb\"))\n",
    "    # dump(clf, './models/rf_f1_9371.joblib')\n",
    "    experiment_path = f\"{root}/experiments/{experiment_name}\"\n",
    "    vectorizer = pickle.load(\n",
    "        open(f\"{experiment_path}/vectorizer.pickle\", \"rb\"))\n",
    "    clf = pickle.load(open(f\"{experiment_path}/classifier.pickle\", 'rb'))\n",
    "    model_dict = {'classifier': clf, 'vectorizer': vectorizer}\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "# LOAD THE EXPERIMENT ARTIFACTS\n",
    "# model_dict = load_model(experiment_name)\n",
    "# sample_vector = model_dict['vectorizer'].transform(['This raise complaint'])\n",
    "# sample_predic = model_dict['classifier'].predict(sample_vector)\n",
    "# sample_predic\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e490422cd5eb80d883ca3c84d0761f55a6ef3e6322675b57b784cf7be9fc5aec"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('venv_brand_ml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
