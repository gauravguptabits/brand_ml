{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npip install tensorflow-addons\\npip install tensorflow-hub\\npip install tensorflow\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pip install tensorflow-addons\n",
    "pip install tensorflow-hub\n",
    "pip install tensorflow\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-19 14:24:18.880533: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-19 14:24:18.880573: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.7.0\n",
      "Eager mode:  True\n",
      "Hub version:  0.12.0\n",
      "GPU is NOT AVAILABLE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-19 14:24:22.216247: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-01-19 14:24:22.216291: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-01-19 14:24:22.216309: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (IBS-LAP-314): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-19 14:24:22.606450: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Split the training set into 60% and 40% to end up with 15,000 examples\n",
    "# for training, 10,000 examples for validation and 25,000 examples for testing.\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, validation_data, test_data = tfds.load(name=\"imdb_reviews\", split=('train[:60%]', 'train[60%:]', 'test'), as_supervised=True)\n",
    "train_examples_batch, train_labels_batch = next(iter(train_data.batch(5)))\n",
    "train_examples_batch, train_labels_batch\n",
    "\n",
    "\n",
    "df = pd.read_json('../dataset/data.json', lines=True)\n",
    "df = df.replace({'Complaint': {'Yes': 1, 'No': 0}})\n",
    "X = df['text']\n",
    "y = df['Complaint']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "test_data = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "validation_data = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "29/29 [==============================] - 1s 8ms/step - loss: 0.5242 - recall_24: 0.1514 - precision_24: 0.1564 - val_loss: 0.3405 - val_recall_24: 0.0000e+00 - val_precision_24: 0.0000e+00\n",
      "Epoch 2/15\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.3048 - recall_24: 0.0080 - precision_24: 1.0000 - val_loss: 0.2695 - val_recall_24: 0.0120 - val_precision_24: 1.0000\n",
      "Epoch 3/15\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.2543 - recall_24: 0.1594 - precision_24: 0.6667 - val_loss: 0.2280 - val_recall_24: 0.3012 - val_precision_24: 0.7576\n",
      "Epoch 4/15\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.2261 - recall_24: 0.4661 - precision_24: 0.7358 - val_loss: 0.2061 - val_recall_24: 0.4458 - val_precision_24: 0.7872\n",
      "Epoch 5/15\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1986 - recall_24: 0.5299 - precision_24: 0.7151 - val_loss: 0.1890 - val_recall_24: 0.4578 - val_precision_24: 0.8261\n",
      "Epoch 6/15\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1818 - recall_24: 0.6853 - precision_24: 0.7478 - val_loss: 0.1781 - val_recall_24: 0.5422 - val_precision_24: 0.8654\n",
      "Epoch 7/15\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1716 - recall_24: 0.6932 - precision_24: 0.7768 - val_loss: 0.1702 - val_recall_24: 0.6145 - val_precision_24: 0.8644\n",
      "Epoch 8/15\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1548 - recall_24: 0.7211 - precision_24: 0.7939 - val_loss: 0.1669 - val_recall_24: 0.6265 - val_precision_24: 0.8667\n",
      "Epoch 9/15\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1579 - recall_24: 0.7291 - precision_24: 0.7754 - val_loss: 0.1623 - val_recall_24: 0.6506 - val_precision_24: 0.8571\n",
      "Epoch 10/15\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1512 - recall_24: 0.7450 - precision_24: 0.7792 - val_loss: 0.1576 - val_recall_24: 0.6747 - val_precision_24: 0.8485\n",
      "Epoch 11/15\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.1493 - recall_24: 0.7530 - precision_24: 0.8077 - val_loss: 0.1584 - val_recall_24: 0.6627 - val_precision_24: 0.9016\n",
      "Epoch 12/15\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1459 - recall_24: 0.7570 - precision_24: 0.7917 - val_loss: 0.1529 - val_recall_24: 0.6988 - val_precision_24: 0.8406\n",
      "Epoch 13/15\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1352 - recall_24: 0.7649 - precision_24: 0.8205 - val_loss: 0.1547 - val_recall_24: 0.6988 - val_precision_24: 0.8657\n",
      "Epoch 14/15\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1334 - recall_24: 0.7928 - precision_24: 0.8156 - val_loss: 0.1507 - val_recall_24: 0.7108 - val_precision_24: 0.8806\n",
      "Epoch 15/15\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1277 - recall_24: 0.7928 - precision_24: 0.8156 - val_loss: 0.1504 - val_recall_24: 0.6867 - val_precision_24: 0.8769\n",
      "10/10 - 0s - loss: 0.1736 - recall_24: 0.6429 - precision_24: 0.8571 - 18ms/epoch - 2ms/step\n",
      "loss: 0.174\n",
      "recall_24: 0.643\n",
      "precision_24: 0.857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.825897731881571"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import fbeta_score, precision_score, recall_score\n",
    "import tensorflow_addons as tfa\n",
    "import datetime\n",
    "embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "# embedding = 'https://tfhub.dev/google/Wiki-words-500-with-normalization/2'\n",
    "embed_dim = 50\n",
    "lstm_out = 196\n",
    "max_features=100\n",
    "\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=False)\n",
    "# hub_layer(train_examples_batch[:3])\n",
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "# model.add(tf.keras.layers.Embedding(max_features, embed_dim, input_length=50))\n",
    "# model.add(tf.keras.layers.LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "fBetaMetric = tfa.metrics.FBetaScore(num_classes=1, beta=2.0, threshold=0.5)\n",
    "Precision = tf.keras.metrics.Precision(),\n",
    "Recall = tf.keras.metrics.Recall()\n",
    "\n",
    "model.compile(  optimizer='adam', \n",
    "                loss='binary_crossentropy',\n",
    "                metrics=[ Recall, Precision ])\n",
    "# model.summary()\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE_SIZE = 32\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "history = model.fit(\n",
    "    train_data.shuffle(SHUFFLE_SIZE).batch(BATCH_SIZE),\n",
    "    epochs=15,\n",
    "    validation_data=validation_data.batch(BATCH_SIZE),\n",
    "    # callbacks=[tensorboard_callback],\n",
    "    verbose=1)\n",
    "\n",
    "results = model.evaluate(test_data.batch(BATCH_SIZE), verbose=2)\n",
    "\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "  print(\"%s: %.3f\" % (name, value))\n",
    "\n",
    "y_test_true = [ty.numpy() for (tx, ty) in test_data]\n",
    "y_test_pred = np.round(model.predict(test_data.batch(BATCH_SIZE)))\n",
    "fbeta_score(y_test_true, y_test_pred, average='macro', beta=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../experiments/lstm_frozen/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../experiments/lstm_frozen/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8571428571428571\tRecall: 0.6428571428571429\tFbeta: 0.825897731881571\n"
     ]
    }
   ],
   "source": [
    "model.save('../experiments/lstm_frozen')\n",
    "savedModel = tf.keras.models.load_model('../experiments/lstm_frozen')\n",
    "# savedModel.predict(test_data.batch(BATCH_SIZE))\n",
    "y_test_true = [ty.numpy() for (tx, ty) in test_data]\n",
    "y_test_pred = np.round(savedModel.predict(test_data.batch(BATCH_SIZE)))\n",
    "fbeta = fbeta_score(y_test_true, y_test_pred, average='macro', beta=2)\n",
    "prec = precision_score(y_test_true, y_test_pred)\n",
    "recall = recall_score(y_test_true, y_test_pred)\n",
    "\n",
    "print(f'Precision: {prec}\\tRecall: {recall}\\tFbeta: {fbeta}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1819ded1536e1a7833f567d5bd67996212761a707707d2444676b6e97db37213"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('venv_tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
