{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade numpy\n",
    "# ! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Word2Vec model.\n",
    "# w2v_model = gensim.models.Word2Vec.load(\"./results/model.w2v\")\n",
    "\n",
    "\n",
    "# def vector(word, w2v_model):\n",
    "#     returan w2v_model.wv.key_to_index[word]\n",
    "\n",
    "\n",
    "import gensim\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "\n",
    "# WORD2VEC\n",
    "W2V_SIZE = 300\n",
    "W2V_WINDOW = 7\n",
    "W2V_EPOCH = 32\n",
    "W2V_MIN_COUNT = 10\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "# Load pre-trained Word2Vec model.\n",
    "\n",
    "\n",
    "def train_w2v_model(docs, vector_size=W2V_SIZE, window=W2V_WINDOW, min_count=W2V_MIN_COUNT, epochs=W2V_EPOCH):\n",
    "    w2v_model = gensim.models.word2vec.Word2Vec(vector_size=vector_size,\n",
    "                                                window=window,\n",
    "                                                min_count=min_count,\n",
    "                                                workers=10)\n",
    "    tokenized_docs = [doc.split() for doc in docs]\n",
    "    w2v_model.build_vocab(tokenized_docs)\n",
    "\n",
    "    vocab_size = len(w2v_model.wv)\n",
    "    print(\"Vocab size\", vocab_size)\n",
    "    w2v_model.train(tokenized_docs, total_examples=len(tokenized_docs), epochs=epochs)\n",
    "\n",
    "    return w2v_model\n",
    "\n",
    "\n",
    "def create_document_vector(tokens, size, model_w2v):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v.wv[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:  # handling the case where the token is not in vocabulary\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "\n",
    "def create_documents_vectors(documents, w2v_model, size=W2V_SIZE):\n",
    "    TOTAL_DOCS = documents.shape[0]\n",
    "    wordvec_arrays = np.zeros((TOTAL_DOCS, size))\n",
    "\n",
    "    for i in range(TOTAL_DOCS):\n",
    "        wordvec_arrays[i, :] = create_document_vector(documents[i], size, w2v_model)\n",
    "    wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "    return wordvec_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "\n",
    "def add_label(documents):\n",
    "    output = []\n",
    "    tokenized_docs = [doc.split() for doc in documents]\n",
    "\n",
    "    for i, s in zip(list(documents.index), tokenized_docs):\n",
    "        output.append(TaggedDocument(s, [\"tweet_\" + str(i)]))\n",
    "    return output\n",
    "\n",
    "def train_doc_model(documents, vector_size=200):\n",
    "    TOTAL_DOCS = len(documents)\n",
    "    print('TOTAL Docs v1')\n",
    "    model_d2v = gensim.models.Doc2Vec(dm=0,  # dm = 1 for ‘distributed memory’ model\n",
    "                                    dm_mean=1,  # dm_mean = 1 for using mean of the context word vectors\n",
    "                                    vector_size=vector_size,  # no. of desired features\n",
    "                                    window=5,  # width of the context window\n",
    "                                    negative=6,  # if > 0 then negative sampling will be used\n",
    "                                    # Ignores all words with total frequency lower than 5.\n",
    "                                    min_count=3,\n",
    "                                    workers=32,  # no. of cores\n",
    "                                    alpha=0.1,  # learning rate\n",
    "                                    seed=23,  # for reproducibility\n",
    "                                    )\n",
    "    labeled_tweets = add_label(documents)  # label all the tweets\n",
    "    model_d2v.build_vocab([i for i in labeled_tweets])\n",
    "    model_d2v.train(labeled_tweets, total_examples=TOTAL_DOCS, epochs=25)\n",
    "    return model_d2v\n",
    "\n",
    "def create_documents_vectors_from_Doc2Vec(documents, model_d2v, vector_size=200):\n",
    "    TOTAL_DOCS = len(documents)\n",
    "    docvec_arrays = np.zeros((TOTAL_DOCS, vector_size))\n",
    "    for i in range(TOTAL_DOCS):\n",
    "        docvec_arrays[i, :] = model_d2v.docvecs[i].reshape((1, vector_size))\n",
    "    return pd.DataFrame(docvec_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def to_tf_idf_vector(df):\n",
    "    tfidf = TfidfVectorizer(lowercase=False, max_df=0.8, min_df=0.01, ngram_range=(\n",
    "        1, 3))  # Most of the terms are eliminated by min_df\n",
    "    vectors = tfidf.fit_transform(df['text']).toarray()  # 2\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# %run ./preprocess.ipynb\n",
    "\n",
    "# ds_root = '/home/gaurav.gupta/projects/PoCs/brandMention/brand_datasets/'\n",
    "# r_path = os.path.join(ds_root, 'ds_complaints',\n",
    "#                       'panasonic_random_sample_predicted.csv')\n",
    "# r_df = read_file(r_path)\n",
    "\n",
    "# m_path = os.path.join(ds_root, 'ds_complaints', 'panasonic_v1_g.csv')\n",
    "# m_df = read_file(m_path)\n",
    "\n",
    "# raw_df = pd.concat([r_df, m_df])\n",
    "# raw_df.reset_index(drop=True, inplace=True)\n",
    "# df = process_data(raw_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model = train_w2v_model(df)\n",
    "# create_document_vector()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e490422cd5eb80d883ca3c84d0761f55a6ef3e6322675b57b784cf7be9fc5aec"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('venv_brand_ml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
